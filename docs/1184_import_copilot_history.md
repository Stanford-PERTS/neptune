# #1184: importing copilot history to `participant_data`

I imported a bunch of production data, and a sql-ized version of Qualtrics responses, to create an INSERT query that can create `progress` rows for every response we have in qualtrics.

```r
# Downloaded from Qualtrics.
q_raw <- read.csv('~/Downloads/Engagement_Project_OFFICIAL_20182019.csv', stringsAsFactors = FALSE)

# Fix the V1-V10 column labeling
q_csv <- q_raw
q_names <- names(q_csv)
q_names <- c(unlist(q_csv[1, 1:10]), q_names[11:length(q_names)])
names(q_csv) <- q_names

# Throw away the second row of column headers
q_csv <- q_csv[2:nrow(q_csv), ]

# Can import this as a sql table with SequelPro.
write.csv(q_csv, '~/Downloads/ep_response.csv')
```

Then copied production data into new tables:

* classroom (from triton)
* cycle (from triton)
* ep_response (from Q csv)
* project_cohort (from BigQuery)

Then updated existing rows which matched a cycle (by date) so they had the new-style survey id. This query takes a long time.

```sql
UPDATE `participant_data` pd
INNER JOIN (
  SELECT
    pd.`uid` as `participant_data_id`,
    CONCAT(
      pd.`survey_id`,
      ':cycle-',
      CAST(cy.`ordinal` AS CHAR)
    ) as `survey_id`,
    cy.`ordinal` as `survey_ordinal`
  FROM `participant_data` pd
  JOIN `classroom` cl
    ON pd.`code` = cl.`code`
  JOIN `cycle` cy
    ON (
      cy.`team_id` = cl.`team_id` AND
      pd.`modified` > CONCAT(cy.`start_date`, ' 00:00:00') AND
      pd.`modified` <= CONCAT(cy.`end_date`, ' 23:59:59')
    )
) t1
ON pd.`uid` = t1.`participant_data_id`
SET
  pd.`modified` = pd.`modified`,
  pd.`survey_id` = t1.`survey_id`,
  pd.`survey_ordinal` = t1.`survey_ordinal`
WHERE
  pd.`program_label` = 'triton' AND
  t1.`survey_ordinal` IS NOT NULL;
```

That updated about 34k rows. Then insert new rows based on Qualtrics responses. About 3.2k rows are generated by the SELECT, but after the IGNORE only about 1.4k are inserted.

```sql
INSERT IGNORE INTO participant_data
SELECT
  CONCAT('ParticipantData_', SUBSTRING(MD5(RAND()) FROM 1 FOR 16)) as uid,
  # This field will have to be corrected by a later UPDATE query.
  'import' as `short_uid`,
  q.`StartDate` as `created`,
  q.`StartDate` as `modified`,
  'progress' as `key`,
  q.`progress` as `value`,
  q.`participant_id` as `participant_id`,
  'triton' as `program_label`,
  NULL as `project_id`,
  NULL as `cohort_label`,
  pc.`uid` as `project_cohort_id`,
  q.`code` as `code`,
  IF(
    cy.`uid` IS NULL,
    q.`survey_id`,
    CONCAT(
      q.`survey_id`,
      ':cycle-',
      CAST(cy.`ordinal` AS CHAR)
    )
  ) as `survey_id`,
  IF(cy.`uid` IS NULL, 1, cy.`ordinal`) as `survey_ordinal`,
  0 as `testing`
FROM `ep_response` q
JOIN `classroom` cl
  ON q.`code` = cl.`code`
LEFT OUTER JOIN `cycle` cy
  ON (
    cy.`team_id` = cl.`team_id` AND
    q.`StartDate` > CONCAT(cy.`start_date`, ' 00:00:00') AND
    q.`StartDate` <= CONCAT(cy.`end_date`, ' 23:59:59')
  )
JOIN `project_cohort` pc
  ON q.`code` = pc.`code`;
```

And finally, UPDATE `short_uid` to match the random uids, being careful to preserve the modified time:

```sql
UPDATE `participant_data`
SET
  `modified` = `modified`,
  `short_uid` = SUBSTRING_INDEX(`uid`, '_', -1)
WHERE `short_uid` = 'import';
```

----

Now cleaning up the little window between my import and when pd started getting their descriptor recorded correctly. The basic idea is to run the UPDATE query again, but limit it to a much smaller set so it runs fast.

First running a SELECT version so I can get a sense of what it's doing.

```sql
SELECT
  pd.`uid` as `participant_data_id`,
  CONCAT(
    pd.`survey_id`,
    ':cycle-',
    CAST(cy.`ordinal` AS CHAR)
  ) as `survey_id`,
  cy.`ordinal` as `survey_ordinal`
FROM `participant_data` pd
JOIN `classroom` cl
  ON pd.`code` = cl.`code`
JOIN `cycle` cy
  ON (
    cy.`team_id` = cl.`team_id` AND
    pd.`modified` > CONCAT(cy.`start_date`, ' 00:00:00') AND
    pd.`modified` <= CONCAT(cy.`end_date`, ' 23:59:59')
  )
WHERE
  pd.`program_label` = 'triton' AND
  pd.`modified` > '2019-05-02' AND
  pd.`modified` < '2019-05-04';
```

This should correct most of the pd that were recorded for triton in this time period (1246/1283). I can stop at 2019-05-03 22:20:38 because I can see myself testing it at that time (paricipant 123 in Arnrow team):

|        participant_id        |   key    |        survey_id        |       modified      |
|------------------------------|----------|-------------------------|---------------------|
| Participant_XtTxLkjHgL0a8cgJ | progress | Survey_hIGNVOsc         | 2019-05-03 22:21:14 |
| Participant_XtTxLkjHgL0a8cgJ | progress | Survey_hIGNVOsc:cycle-1 | 2019-05-03 22:20:38 |

But also, I can add `IGNORE` to have it skip over any index collisions it creates. Since, in those cases, the survey ID is already correct, they'll be fine to skip. Final update query:

```sql
UPDATE IGNORE `participant_data` pd
INNER JOIN (
  SELECT
    pd.`uid` as `participant_data_id`,
    CONCAT(
      pd.`survey_id`,
      ':cycle-',
      CAST(cy.`ordinal` AS CHAR)
    ) as `survey_id`,
    cy.`ordinal` as `survey_ordinal`
  FROM `participant_data` pd
  JOIN `classroom` cl
    ON pd.`code` = cl.`code`
  JOIN `cycle` cy
    ON (
      cy.`team_id` = cl.`team_id` AND
      pd.`modified` > CONCAT(cy.`start_date`, ' 00:00:00') AND
      pd.`modified` <= CONCAT(cy.`end_date`, ' 23:59:59')
    )
  WHERE
    pd.`modified` > '2019-05-02' AND
    pd.`modified` < '2019-05-03 22:20:38'
) t1
ON pd.`uid` = t1.`participant_data_id`
SET
  pd.`modified` = pd.`modified`,
  pd.`survey_id` = t1.`survey_id`,
  pd.`survey_ordinal` = t1.`survey_ordinal`
WHERE
  pd.`program_label` = 'triton' AND
  t1.`survey_ordinal` IS NOT NULL AND
  pd.`modified` > '2019-05-02' AND
  pd.`modified` < '2019-05-03 22:20:38'
```

1138 rows affected in production, once run. This query confirms that lots of pd now have the survey descriptor:

```sql
SELECT
  `uid`,
  `survey_id`,
  `modified`
FROM `participant_data`
WHERE
  `program_label` = 'triton' AND
  `modified` > '2019-05-02' AND
  `modified` < '2019-05-04';
```

----


SECOND TIME! This is after I realized that I only had the first thousand project cohorts, so a bunch of data was excl uded. I've gotten fresh copies of `classroom` and `cycle` and `project_cohort`, but I haven't bothered getting a fresh copy of `ep_response` since it's data in the past I'm worried about.

The select version of this query results now in 15,715 rows, and the insert creates... 5,458 _new_ rows, so combined from last time that's a total import of about 6.9k rows.
